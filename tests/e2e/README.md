# E2E Tests for BMAD Skills

## ⚠️ Important: Real Claude CLI Tests

These tests are **NOT mocks** - they call the real `claude` CLI and activate actual skills.

### What Happens When Tests Run

1. **Real API Calls**
   ```python
   # This actually calls: claude -p --output-format json --allowedTools "Skill Read Write Grep"
   response = claude_client.execute("I have an idea for a todo app")
   ```

2. **Skills Activate**
   - Skills are loaded from `.claude/skills/`
   - Claude processes the prompt
   - Appropriate skill activates (e.g., `bmad-discovery-research`)
   - Skill may generate artifacts

3. **Files Created**
   ```
   .claude/skills/_runtime/workspace/
   ├── artifacts/
   │   ├── PRD.md                    # ✅ Generated by tests
   │   ├── discovery-brief.md        # ✅ Generated by tests
   │   └── architecture-decisions.md # ✅ Generated by tests
   ├── stories/
   │   └── story-001.md              # ✅ Generated by tests
   └── changes/
       └── change-xyz/               # ✅ Generated by tests
   ```

   **PLUS:**
   ```
   docs/
   ├── PRD-TodoApp.md               # ✅ Sometimes created by skills
   └── epics-TodoApp.md             # ✅ Sometimes created by skills
   ```

4. **Automatic Cleanup** ✅
   - After each test completes, all generated files are **automatically deleted**
   - The `cleanup_runtime_workspace` fixture handles this
   - Prevents git pollution and test interference

---

## 🧹 Cleanup Behavior

### What Gets Cleaned

The `cleanup_runtime_workspace` fixture (in `conftest.py`) automatically removes:

- ✅ `artifacts/*.md` (except `.template` files)
- ✅ `stories/*.md` (all markdown files)
- ✅ `changes/*/` (entire OpenSpec change directories)
- ✅ `docs/` (any test-generated documentation)

### What's Preserved

- ✅ `.template` files (never touched)
- ✅ `_core/`, `_config/`, `_docs/` directories (skill infrastructure)
- ✅ `_runtime/workspace/` structure (directories remain)

### Manual Cleanup (if needed)

If tests crash or you kill them mid-run, cleanup may not happen:

```bash
# Clean manually
rm -rf .claude/skills/_runtime/workspace/artifacts/*.md
rm -rf .claude/skills/_runtime/workspace/stories/*.md
rm -rf .claude/skills/_runtime/workspace/changes/*
rm -rf docs/
```

---

## 💰 Cost Implications

**These tests cost real money!**

| Test Type | Approx. Cost | Duration |
|-----------|-------------|----------|
| Single smoke test | $0.10-0.30 | 1-2 min |
| Full BMAD workflow | $0.50-1.00 | 5-10 min |
| Complete E2E suite | $1-5 | 20-30 min |

### Cost Control

1. **Use smoke tests for quick validation:**
   ```bash
   npm run test:e2e:smoke  # Only 2-3 tests, ~$0.30
   ```

2. **Run full suite sparingly:**
   ```bash
   npm run test:e2e  # All tests, ~$1-5
   ```

3. **Check cost after running:**
   - Tests print total cost at end
   - Example: `Total cost: $1.2345`

---

## 🚀 Running Tests

### Prerequisites

1. **Claude Max subscription** with `claude` CLI installed
2. **Skills installed** globally or locally:
   ```bash
   bash scripts/install-to-home.sh
   # OR
   bash scripts/install-to-project.sh
   ```
3. **Python dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

### Run Commands

```bash
# Quick smoke test (~2 min, ~$0.30)
npm run test:e2e:smoke

# Full E2E suite (~30 min, ~$1-5)
npm run test:e2e

# Specific workflow
npm run test:e2e:bmad       # BMAD only
npm run test:e2e:openspec   # OpenSpec only

# Single test file
python3 -m pytest tests/e2e/test_bmad_workflows.py -v

# Single test
python3 -m pytest tests/e2e/test_bmad_workflows.py::test_new_idea_activates_discovery -v -s
```

---

## 🔍 Debugging Failed Tests

### Check Cleanup Logs

Cleanup prints what it removes:
```
[Cleanup] Removed: .claude/skills/_runtime/workspace/artifacts/PRD.md
[Cleanup] Removed: docs/PRD-TodoApp.md
```

### Inspect Generated Files (Before Cleanup)

Add a breakpoint or sleep before cleanup:
```python
def test_prd_creation(session_manager):
    response = session_manager.execute_turn(session, "Create a PRD")

    import time
    time.sleep(60)  # Wait 60s to inspect files

    # Files will be cleaned up after test completes
```

### Skip Cleanup for Debugging

Modify `conftest.py` temporarily:
```python
@pytest.fixture(autouse=True)
def cleanup_runtime_workspace(runtime_workspace, request):
    yield

    # TEMPORARILY DISABLE CLEANUP FOR DEBUGGING
    return  # <-- Add this line

    if 'e2e' in request.keywords:
        # ... cleanup code ...
```

### Check Skill Activation

If test fails with "skill not detected":
1. Check permission denials: `jq '.permission_denials' response.json`
2. Check turn count: `jq '.num_turns' response.json` (should be > 1)
3. Check allowed tools: Ensure `Skill` is in `--allowedTools`

---

## 📊 Test Output Example

```bash
$ npm run test:e2e:smoke

======================================================================
Test: test_new_idea_activates_discovery
======================================================================
[ClaudeClient] Executing: claude -p --output-format json 'I have an idea...'
[ClaudeClient] Response: 1222 chars, $0.2877, 60370ms
✅ Discovery phase mentioned
✅ Skill behavior keywords found
[Cleanup] Removed: .claude/skills/_runtime/workspace/artifacts/discovery-brief.md
PASSED

======================================================================
E2E Test Run Summary
======================================================================
Total API calls: 3
Total cost: $0.8234
Average cost per call: $0.2745
======================================================================

✅ 3 passed in 156.23s
```

---

## ⚠️ Known Limitations

### 1. Batch Mode vs Interactive

Skills behave slightly differently in `claude -p` (batch) vs interactive mode:
- Batch: Skills may need explicit permissions
- Interactive: More natural activation

**Solution:** Tests use `--allowedTools "Skill Read Write Grep"` by default.

### 2. Non-Deterministic

LLM responses aren't 100% deterministic:
- Same prompt may activate different skill occasionally
- Content may vary slightly

**Solution:** Tests use multi-level validation (keywords + turn count + artifacts).

### 3. Slow Execution

E2E tests are slow (real API calls):
- Single test: 1-2 minutes
- Full suite: 20-30 minutes

**Solution:** Use smoke tests for quick validation, full suite for CI/nightly.

---

## 🔧 Maintenance

### Adding New Tests

1. Follow existing patterns in `test_*_workflows.py`
2. Use `@pytest.mark.e2e` marker
3. Use `@pytest.mark.smoke` for quick tests
4. Use `@pytest.mark.expensive` if cost > $0.50
5. Cleanup is automatic - don't worry about it!

Example:
```python
@pytest.mark.e2e
@pytest.mark.smoke
def test_my_new_skill(claude_client, output_validator):
    response = claude_client.execute("My prompt")

    assert output_validator.validate_skill_activation(
        response,
        expected_skill="my-skill-name"
    )

    # No cleanup needed - automatic! ✅
```

### Updating Skill Signatures

If skills change behavior, update keyword signatures in:
- `helpers/claude_client.py` (SKILL_SIGNATURES)
- `helpers/skill_verifier.py` (SKILL_ARTIFACTS)

---

## 📚 Related Documentation

- **SKILL_VERIFICATION.md** - Detailed guide on skill verification methods
- **IMPROVEMENTS.md** - Roadmap for improving E2E tests (pexpect, mocks, etc.)
- **../CLAUDE.md** - Project-level testing documentation

---

**Remember:** These tests use real Claude API and cost real money. Run wisely! 💰
