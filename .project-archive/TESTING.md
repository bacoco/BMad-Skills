# StratÃ©gie de Test BMAD Skills

## ğŸ¯ Philosophie

BMAD Skills est un systÃ¨me **conversationnel interactif**. La stratÃ©gie de test reflÃ¨te cette rÃ©alitÃ© :
- **90% automatisÃ©s** : Ce qui est dÃ©terministe (structure, metadata, scripts)
- **10% manuels** : Ce qui est conversationnel (workflows, gÃ©nÃ©ration artefacts)

---

## âœ… Tests AutomatisÃ©s

### Tests Statiques (Actuel : 56 tests âœ…)

```bash
npm test
```

**Ce qu'on teste :**
- âœ… Metadata YAML valide (frontmatter de SKILL.md)
- âœ… CohÃ©rence allowed-tools (SKILL.md â†” MANIFEST.json)
- âœ… Templates prÃ©sents dans assets/
- âœ… Descriptions cohÃ©rentes
- âœ… Version alignment (package.json â†” MANIFEST.json)
- âœ… Structure skills (SKILL.md, REFERENCE.md, WORKFLOW.md, CHECKLIST.md)

**Commandes spÃ©cifiques :**
```bash
pytest tests/test_skill_metadata.py       # Metadata validation
pytest tests/test_manifest_consistency.py # Manifest coherence
pytest tests/test_template_assets.py      # Templates present
```

### Tests Unitaires Python (Ã€ implÃ©menter)

```bash
pytest tests/unit/
```

**Ce qui doit Ãªtre testÃ© :**
- [ ] `workflow_status.py` - Lecture/Ã©criture YAML status
- [ ] `sprint_status.py` - Parsing epics â†’ stories
- [ ] `activation_metrics.py` - Collecte mÃ©triques
- [ ] Gestion d'erreurs (YAML corrompu, fichiers manquants)

**PrioritÃ© P0** selon IMPROVEMENTS.md

---

## ğŸ§ª Tests Manuels Conversationnels

### Pourquoi Manuels ?

**Les workflows BMAD sont conversationnels** : les skills posent des questions, l'utilisateur rÃ©pond, le contexte s'accumule. **Impossible Ã  automatiser.**

Voir `tests/WHY_NO_E2E_TESTS.md` pour l'explication complÃ¨te.

### Workflows Ã€ Tester Manuellement

#### 1. Workflow BMAD Complet (L2-4)

```bash
claude  # Mode interactif
```

**ScÃ©nario :** App de budget pour Ã©tudiants

```
> Je veux crÃ©er une app de budget pour Ã©tudiants

Claude (Discovery): Quels sont les principaux problÃ¨mes de vos utilisateurs ?
> Ils oublient leurs dÃ©penses et dÃ©passent leur budget

Claude (Discovery): Quelles features prioritaires ?
> Tracking dÃ©penses, alertes budget, catÃ©gories

Claude: Compris. Voulez-vous un PRD ?
> Oui

Claude (PM): [gÃ©nÃ¨re PRD.md]
```

**VÃ©rifications :**
- [ ] PRD.md crÃ©Ã© dans `.claude/skills/_runtime/workspace/artifacts/`
- [ ] Contient sections : Goals, Features, Metrics, Requirements
- [ ] Contenu pertinent (mentionne Ã©tudiants, budget, etc.)
- [ ] QualitÃ© : Complet, cohÃ©rent, actionnable

**Continuer avec :**
```
> Design l'architecture technique

Claude (Architect): [gÃ©nÃ¨re architecture-decisions.md]
```

**VÃ©rifications :**
- [ ] architecture-decisions.md crÃ©Ã©
- [ ] Stack technique dÃ©fini
- [ ] Composants identifiÃ©s
- [ ] Data flow documentÃ©

**Continuer avec :**
```
> Break down en developer stories

Claude (Stories): [gÃ©nÃ¨re story-001.md, story-002.md, etc.]
```

**VÃ©rifications :**
- [ ] story-*.md crÃ©Ã©s dans `.claude/skills/_runtime/workspace/stories/`
- [ ] Chaque story a acceptance criteria
- [ ] Stories couvrent les features du PRD
- [ ] Estimations prÃ©sentes

**RÃ©sultat attendu :** âœ… Workflow complet validÃ©

---

#### 2. Workflow OpenSpec (L0-1)

```
> Fix: Le timeout de session est trop court (15min â†’ 60min)

Claude (OpenSpec Proposal): [gÃ©nÃ¨re proposal.md]
```

**VÃ©rifications :**
- [ ] proposal.md crÃ©Ã© dans `.claude/skills/_runtime/workspace/changes/`
- [ ] Problem statement clair
- [ ] Solution proposÃ©e
- [ ] Tasks.md avec tÃ¢ches

```
> ImplÃ©mente ce changement

Claude (OpenSpec Implementation): [gÃ©nÃ¨re execution-log.md]
```

**VÃ©rifications :**
- [ ] execution-log.md mis Ã  jour
- [ ] Change implÃ©mentÃ© ou documentÃ©
- [ ] Status actualisÃ©

---

### FrÃ©quence Tests Manuels

- **Avant chaque release majeure** : Workflow BMAD complet
- **Avant chaque release mineure** : Workflow OpenSpec
- **Ad-hoc** : Si changement majeur dans un skill

### Documentation Tests Manuels

**Format :**
```markdown
## Test Manuel - [Date]

**Testeur :** [Nom]
**Version :** [X.Y.Z]
**Workflow :** BMAD Complet

### RÃ©sultats

- âœ… Discovery â†’ PRD : OK (PRD.md gÃ©nÃ©rÃ©, qualitÃ© bonne)
- âœ… PRD â†’ Architecture : OK (architecture-decisions.md complet)
- âœ… Architecture â†’ Stories : OK (8 stories gÃ©nÃ©rÃ©es)
- âŒ Bug dÃ©couvert : [Description si applicable]

### Artefacts GÃ©nÃ©rÃ©s

- PRD.md (1234 bytes)
- architecture-decisions.md (2345 bytes)
- story-001.md â†’ story-008.md

### Conclusion

âœ… Workflow validÃ©, fonctionne correctement
```

**Fichier :** `tests/manual/test-results-YYYY-MM-DD.md`

---

## ğŸš« Ce Qu'On NE Teste PAS (Et Pourquoi)

### Tests E2E AutomatisÃ©s âŒ

**Pourquoi impossible :**
- Skills sont conversationnels (multi-tours, questions/rÃ©ponses)
- Mode batch `claude -p` ne permet pas l'interaction
- Pas de mock possible (skills chargÃ©s par Claude, pas Python)
- RÃ©sultats non-dÃ©terministes (LLM)

**Voir :** `tests/WHY_NO_E2E_TESTS.md` pour dÃ©tails complets

### Tests d'IntÃ©gration Skills âŒ

**Pourquoi impossible :**
- Transitions entre skills nÃ©cessitent conversation
- Context cumulatif pas scriptable
- Handoffs basÃ©s sur contenu conversationnel

### Tests de RÃ©gression UI âŒ

**Pourquoi non applicable :**
- Pas d'UI (CLI conversationnel)
- Interaction textuelle libre
- Pas de "clicks" ou "forms" Ã  tester

---

## ğŸ“Š Couverture de Test

### Actuel

| Type | Couverture | Status |
|------|-----------|--------|
| Metadata/Structure | 100% | âœ… 56 tests |
| Scripts Python | 0% | âš ï¸ Ã€ faire |
| Workflows conversationnels | Manuel | âœ… ValidÃ© |

### Cible (Score 100/100)

| Type | Couverture | Status |
|------|-----------|--------|
| Metadata/Structure | 100% | âœ… |
| Scripts Python | 80%+ | ğŸ”œ P0 |
| Workflows conversationnels | Manuel | âœ… |

**Note :** Couverture "tests automatisÃ©s" ne peut jamais atteindre 100% pour un systÃ¨me conversationnel. **C'est normal et acceptable.**

---

## ğŸ”„ CI/CD

### GitHub Actions (Ã€ implÃ©menter - P0)

```yaml
# .github/workflows/test.yml
name: Tests

on: [push, pull_request]

jobs:
  static-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
      - run: pip install -r requirements.txt
      - run: pytest tests/ -v  # Tests statiques + unitaires

  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - run: python3 .claude/skills/_core/tooling/lint_contracts.py
```

**Ce qui est testÃ© en CI :**
- âœ… Tests statiques (metadata, structure)
- âœ… Tests unitaires Python
- âœ… Linting contracts

**Ce qui N'EST PAS testÃ© en CI :**
- âŒ Workflows conversationnels (impossible Ã  automatiser)

---

## ğŸ“ Checklist Avant Release

### Tests AutomatisÃ©s

- [ ] `npm test` passe (tests statiques)
- [ ] `pytest tests/unit/` passe (tests unitaires)
- [ ] `npm run lint` passe (contracts validation)
- [ ] CI/CD green (si implÃ©mentÃ©)

### Tests Manuels

- [ ] Test workflow BMAD complet (voir section ci-dessus)
- [ ] Test workflow OpenSpec (voir section ci-dessus)
- [ ] RÃ©sultats documentÃ©s dans `tests/manual/`

### Validation QualitÃ©

- [ ] Tous les skills ont SKILL.md, REFERENCE.md, WORKFLOW.md, CHECKLIST.md
- [ ] Templates prÃ©sents dans assets/
- [ ] Scripts Python ont error handling
- [ ] MANIFEST.json synchronisÃ©s (meta/ et _config/)

---

## ğŸ“ Pour les Nouveaux Contributeurs

### "Pourquoi pas de tests E2E ?"

**RÃ©ponse courte :** Impossible, systÃ¨me conversationnel.

**RÃ©ponse longue :** Lire `tests/WHY_NO_E2E_TESTS.md`

### "Comment tester mes changements ?"

1. **Si vous modifiez metadata/structure :**
   ```bash
   npm test  # Doit passer
   ```

2. **Si vous modifiez scripts Python :**
   ```bash
   pytest tests/unit/test_[votre_script].py  # Ã‰crire tests d'abord
   ```

3. **Si vous modifiez un skill (SKILL.md, etc.) :**
   ```bash
   # Test manuel conversationnel
   claude  # Mode interactif
   > [Tester le skill manuellement]
   ```

### "Mon PR peut-il Ãªtre mergÃ© sans tests E2E ?"

**OUI !** Les tests E2E ne sont pas requis (et impossibles).

**Requis :**
- âœ… Tests statiques passent
- âœ… Tests unitaires passent (si code Python modifiÃ©)
- âœ… Test manuel effectuÃ© (si skill modifiÃ©)
- âœ… RÃ©sultats documentÃ©s

---

## ğŸ“š RÃ©fÃ©rences

- **WHY_NO_E2E_TESTS.md** : Explication dÃ©taillÃ©e impossibilitÃ© E2E
- **IMPROVEMENTS.md** : Roadmap avec prioritÃ©s (tests unitaires P0)
- **tests/manual/** : RÃ©sultats des tests manuels conversationnels

---

*StratÃ©gie validÃ©e : Oct 2025*
*Workflows testÃ©s manuellement et fonctionnent correctement* âœ…
